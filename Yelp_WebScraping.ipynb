{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import http.client, urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to save html files\n",
    "def saveString(html, filename):\n",
    "\ttry:\n",
    "\t\tfile = open(filename,\"w\")\n",
    "\t\tfile.write(str(html))\n",
    "\t\tfile.close()\n",
    "\texcept Exception as ex:\n",
    "\t\tprint('Error: ' + str(ex))\n",
    "\n",
    "#Function to load html files\n",
    "def loadString(f):\n",
    "\ttry:\n",
    "\t\thtml = open(f, \"r\", encoding='utf-8').read()\n",
    "\t\treturn(html)\n",
    "\texcept Exception as ex:\n",
    "\t\tprint('Error: ' + str(ex))\n",
    "\n",
    "#To replicate actual user behaviour on a browser\n",
    "header_vars= {\n",
    "\t\t\"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "\t\t\"accept-encoding\": \"gzip, deflate, br\",\n",
    "\t\t\"accept-language\": \"en-GB,en-US;q=0.9,en;q=0.8\",\n",
    "\t\t\"cache-control\": \"max-age=0\",\n",
    "\t\t\"cookie\": \"__uzma=656e04b8-689b-4af5-956e-ecc0e52d0459; __uzmb=1642384927; __uzmc=508531043362; __uzmd=1642384927; __uzme=6174; __uzmf=7f3000f54b972e-7a1f-4f3e-8597-c9b82815fc4191fc0b70a70e438410; __ssds=2; __ssuzjsr2=a9be0cd8e; __uzmaj2=3071bfbf-a34d-454c-a9b3-ce974951c0bb; __uzmbj2=1642384928; __uzmcj2=580381076551; __uzmdj2=1642384928; ak_bmsc=18A13C834C6EE2297609BC196B251B93~000000000000000000000000000000~YAAQzXpCF5+0heN9AQAAgTJcag73FiKCg4/zBZQHKJ+UoBBL+UD4HclUDuPWnRmkeXOdTx7A0Rrgy+3RzouXPgdTiNrAgTfzwNECXPOYfT0aza4l8dBHR2ym+XRTO+wWXJZwspKHBSt4ikMFUSogWEHrRfyhD9KnjQkrs66uP3QwL3KIaGK2melsvb3h+2/VfD57hJsjeFRsilniQ8XLQGixWON2epSxOI5EJURojzF3VXNqZr79lSq+NicYSDY8aU08HjC3YtotIiJ8eguuZv5HxjDPFbs1BbBpUYodxKqisLbu1eEYPCgYqARd6Ry13MBzIJjtDhMR69JJK6dLXC4c1weH+sBsPvaqA9IhR9yXVVePguSlrcluE5uG/eEbl1aVe85fLw==; s=CgAD4ACBh51A0NjVjNTIxZWUxN2UwYWI4Y2JiMDRmMjU4ZmZmNmU4ZDmQ+mTx; npii=btguid/65c521ee17e0ab8cbb04f258fff6e8d965a865fb^cguid/65c5266817e0ab8cc1224a83ff6a881965a865fb^; ebay=%5Esbf%3D%23000000%5E; dp1=bu1p/QEBfX0BAX19AQA**65a865fb^bl/US65a865fb^; nonsession=BAQAAAX42/BiUAAaAADMABWPHMns5NDEyMgDKACBlqGX7NjVjNTIxZWUxN2UwYWI4Y2JiMDRmMjU4ZmZmNmU4ZDkAywACYeYGAzE0itmBEsw1q38j8xYAykLMK73No8c*; bm_sv=0601DA57A7CA54406136CDCACF44DF42~MSyqjSDnWVhHfr3JjTO7k1sCODPgV4YV2GZjV2GAGLd4fUNZybtHajYnmslko8iQ/26HcsXohlAlwP850fnv4hTSsYycvRBVE/jmVQV4/aTXjODnySUtVq/5Etyuz8mqk1Ww6qR0s8BC8V5YKLDSaw==\",\n",
    "\t\t\"sec-ch-ua-mobile\": \"?0\",\n",
    "\t\t\"sec-ch-ua-platform\": \"macOS\",\n",
    "\t\t\"sec-fetch-dest\": \"document\",\n",
    "\t\t\"sec-fetch-mode\": \"navigate\",\n",
    "\t\t\"sec-fetch-site\": \"none\",\n",
    "\t\t\"sec-fetch-user\": \"?1\",\n",
    "\t\t\"upgrade-insecure-requests\": \"1\",\n",
    "\t\t\"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\"\n",
    "\t\t}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)  Yelp uses GET requests for its search. Write a program that searches on yelp.com for the top-40 “Donut Shop” in the San Francisco area. Save each search result page to disk, “sf_donut_shop_search_page_[PN].htm”**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.yelp.com/search?find_desc=Donut+Shop&find_loc=San+Francisco%2C+CA&start=\"\n",
    "\n",
    "#Create an empty list to store urls\n",
    "list = []\n",
    "pages = [\"0\", \"10\", \"20\", \"30\"]\n",
    "\n",
    "\n",
    "for pageNum in pages:\n",
    "\tpage = requests.get(URL + pageNum, headers = header_vars)\n",
    "    \n",
    "    # Create a beautifulsoup object\n",
    "\tsoup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    #Downloading each page in .htm format\n",
    "\tsaveString(soup, \"sf_donut_shop_search_page_\"+str(pages.index(pageNum)+1)+\".htm\")\n",
    "    \n",
    "    #Wait after downloading each page\n",
    "\tsleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2) Write new code that opens the search result pages saved in (1) and parses out all shop information (search rank, name, linked URL [this store’s Yelp URL], star rating, number of reviews, store tags, “$” signs, delivery / dine-in tags, and whether you can order through Yelp). Skip all “Sponsored Results”.**\n",
    "\n",
    "**(3) Adjust code in (2) to create a MongoDB collection called “sf_donut_shops” that stores all the extracted shop information, one document for each shop.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3b21eeb7f44a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#Run a loop to print all item details on the page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m#               dictionary = {\"searchRank\":[],\"productName\":[],\"reviews\":[], \"ratingNum\":[], \"priceRange\":[], \"yelpOrder\":[], \"StoreTags\":[], \"deliveryTags\":[]};\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#Connect to MongoDB client\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['sf_donut_shops']\n",
    "col = db['sf_donut_shops']\n",
    "\n",
    "#Iterating through all '.htm' pages to retrieve information from each page\n",
    "for pageNum in pages:\n",
    "    #Download page in htm format\n",
    "\thtmlFile = loadString(\"sf_donut_shop_search_page_\"+str(pages.index(pageNum)+1)+\".htm\")\n",
    "\tsoup2 = BeautifulSoup(htmlFile, 'html.parser')\n",
    "\ttags = soup2.select('div[class*=\"hoverable\"]')\n",
    "    \n",
    "    #Run a loop to print all item details on the page\n",
    "\tfor i in range(2, 12):\n",
    "\t\ttag = tags[i]\n",
    "\n",
    "        ### Search Rank\n",
    "\t\trank = tag.find('h3').find('span').get_text(separator=\" \").split(\".\")[0]      \n",
    "    \n",
    "        ### Shop Yelp URL\n",
    "\t\tlink = \"https://www.yelp.com\" + tag.find('a').get('href')\n",
    "\t\tname = tag.find('h3').find('span').find('a').text\n",
    "\t\tregex1 = re.compile('.*reviewCount.*')\n",
    "\t\treviewCount = tag.find(\"span\", {\"class\" : regex1}).get_text()\n",
    "    \n",
    "\t\tregex2 = re.compile('.*i-stars.*')\n",
    "\t\trating = tag.find(\"div\", {\"class\" : regex2})['aria-label']\n",
    "    \n",
    "\t\tregex3 = re.compile('.*priceRange.*')\n",
    "\n",
    "        ### Order through Yelp\n",
    "\t\torderThroughYelp = False\n",
    "\t\tif tag.select('a[class*=\"platformSearchAction\"]'):\n",
    "\t\t\torderThroughYelp = True\n",
    "    \n",
    "        ### Dollars for restaurant\n",
    "\t\ttry:\n",
    "\t\t\tdollars = tag.find(\"span\", {\"class\" : regex3}).text\n",
    "\t\texcept:\n",
    "\t\t\tdollars = \"\"        \n",
    "\n",
    "\n",
    "        ### Store Tags\n",
    "\t\tbuttons = tag.find_all('button')\n",
    "\t\tstoreTagList = []    \n",
    "\t\tfor i in range(len(buttons)):\n",
    "\t\t\tstoreTags = buttons[i].text\n",
    "\t\t\tstoreTagList.append(storeTags)   \n",
    "    \n",
    "        ### Delivery Tags\n",
    "\t\tdelTagList = []\n",
    "\t\tif tag.find(\"div\", {\"data-testid\": \"TRUSTED_PROPERTY\"}):\n",
    "\t\t\tmydivs = tag.find(\"div\", {\"data-testid\": \"TRUSTED_PROPERTY\"})\n",
    "\t\t\tdelTags = mydivs.find_all('p')    \n",
    "\t\t\tfor i in range(len(delTags)):\n",
    "\t\t\t\tdeliveryTags = delTags[i].find('span').text\n",
    "\t\t\t\tdelTagList.append(deliveryTags)       \n",
    "\n",
    "        ### Delivery Tag Values\n",
    "\t\tdelTagVal = []\n",
    "\t\tif tag.find(\"div\", {\"data-testid\": \"TRUSTED_PROPERTY\"}):\n",
    "\t\t\tmydivs = tag.find(\"div\", {\"data-testid\": \"TRUSTED_PROPERTY\"})\n",
    "\t\t\tvalTags = mydivs.find_all(\"span\", {\"role\": \"img\"})\n",
    "\t\t\tfor i in range(len(valTags)):\n",
    "\t\t\t\tif mydivs.select('span[class*=\"close-v2\"]'):\n",
    "\t\t\t\t\tdelTagVal.append(0)\n",
    "\t\t\t\telif mydivs.select('span[class*=\"checkmark\"]'):\n",
    "\t\t\t\t\tdelTagVal.append(1)\n",
    "    \n",
    "\n",
    "\t\tdelTagDict = dict(zip(delTagList, delTagVal))\n",
    "        \n",
    "        #Printing shop details from Yelp on console\n",
    "\t\tprint(\"Shop Rank: \", rank)\n",
    "\t\tprint(\"Shop Name: \", name)\n",
    "\t\tprint(\"Shop URL: \", link)\n",
    "\t\tprint(\"Rating: \", rating)\n",
    "\t\tprint(\"Number of Reviews: \", reviewCount)\n",
    "\t\tprint(\"Store Tags: \", storeTagList)\n",
    "\t\tprint(\"Price Range: \", dollars)\n",
    "\t\tprint(\"Delivery Tags: \", delTagDict)\n",
    "\t\tprint(\"If you can order through Yelp: \", orderThroughYelp)\n",
    "\t\tprint(\"\\n\")\n",
    "        \n",
    "        #Creating an array for keys (which will be part of a dictionary)\n",
    "\t\tvarList = [\"searchRank\",\"productName\", \"productLink\", \"reviews\", \"ratingNum\", \"priceRange\", \"yelpOrder\", \"StoreTags\", \"deliveryTags\"]\n",
    "\n",
    "        #Creating an array for values of the above keys\n",
    "\t\tparamList = [int(rank), name, link, reviewCount, rating, dollars, orderThroughYelp, storeTagList, delTagDict]\n",
    "        \n",
    "        #Creating  a dictionary of shop details\n",
    "\t\tsequence = dict(zip(varList, paramList))\n",
    "         \n",
    "        #Inserting each row in MongoDB\n",
    "\t\tcol.insert_one(sequence)\n",
    "# \t\tcol.drop()\n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sf_donut_shops collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to MongoDB client\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['sf_donut_shops']\n",
    "col = db['sf_donut_shops']\n",
    "\n",
    "query = {}\n",
    "\n",
    "cursor = col.find(query)\n",
    "try:\n",
    "\tfor doc in cursor:\n",
    "\t\tprint(doc)\n",
    "finally:\n",
    "\tclient.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4) Write a _new_ piece of code that reads all URLs stored in “sf_donut_shops” and download each shop page.  Store the page to disk, “sf_donut_shop_[SR].htm”**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to MongoDB client\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['sf_donut_shops']\n",
    "col = db['sf_donut_shops']\n",
    "\n",
    "query = {}\n",
    "\n",
    "projection = {}\n",
    "projection[\"productLink\"] = 1.0\n",
    "projection[\"_id\"] = 0.0\n",
    "\n",
    "cursor = col.find(query, projection = projection)\n",
    "\n",
    "#Create an empty product list\n",
    "prodLinkList = []\n",
    "\n",
    "try:   \n",
    "\tfor doc in cursor:\n",
    "        #Append shop page urls to product list\n",
    "\t\tprodLinkList.append(doc.get('productLink'))\n",
    "finally:\n",
    "\tclient.close()\n",
    "\n",
    "#Iterating through all urls and downloading shop pages in '.htm' format\n",
    "for prodURL in prodLinkList:\n",
    "\tprodPage = requests.get(prodURL, headers = header_vars)\n",
    "\tsoup3 = BeautifulSoup(prodPage.content, \"html.parser\")\n",
    "\tsaveString(soup3, \"sf_donut_shop_\"+str(prodLinkList.index(prodURL)+1)+\".htm\")\n",
    "\tsleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5) Write new code that reads the 40 shop pages saved in (4) and parses each shop’s address, phone number, and website.**\n",
    "\n",
    "**(6) Sign up for a free account with https://positionstack.com/  Adjust your code in (5) to query each shop address’ geolocation (long, lat). Update each shop document on the MongoDB collection “sf_donut_shops” to contain the shop’s address, phone number, website, and geolocation. Lastly, place an index on the shop’s search rank.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shopInfo = {}\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['sf_donut_shops']\n",
    "col = db['sf_donut_shops']\n",
    "\n",
    "#Iterating through downloaded '.htm' files and retrieving shop details\n",
    "for prodURL in prodLinkList:\n",
    "\trank = prodLinkList.index(prodURL)+1\n",
    "\tprodHtmFile = loadString(\"sf_donut_shop_\"+str(prodLinkList.index(prodURL)+1)+\".htm\")\n",
    "\tsoup4 = BeautifulSoup(prodHtmFile, 'html.parser')\n",
    "\tregex4 = re.compile('css-1vhakgw border--top.*')\n",
    "\tsections = soup4.find_all(\"div\", {\"class\" : regex4})\n",
    "\twhole_text=[i.text for i in sections]\n",
    "    \n",
    "    #Retrieve business website\n",
    "\ttry:\n",
    "\t\twebsite_id=[('Business website' in i) for i in whole_text].index(True)\n",
    "\t\twebsite=whole_text[website_id].replace(\"Business website\",\"\")\n",
    "\texcept:\n",
    "\t\twebsite=''\n",
    "    \n",
    "    #Update website details in MongoDB collection\n",
    "\tcol.update_one({\"searchRank\":rank}, {\"$set\":{\"Shop Website\":website}})\n",
    "    \n",
    "    #Retrieve Shop Phone Number\n",
    "\ttry:\n",
    "\t\tphone_number_id=[('Phone number' in i) for i in whole_text].index(True)\n",
    "\t\tphone_number=whole_text[phone_number_id].replace(\"Phone number\",\"\")\n",
    "\texcept:\n",
    "\t\tphone_number=''\n",
    "\n",
    "    #Update phone number details in MongoDB collection\n",
    "\tcol.update_one({\"searchRank\":rank}, {\"$set\":{\"Shop Phone\":phone_number}})\n",
    "\n",
    "    #Retrieve Shop Address\n",
    "\ttry:\n",
    "\t\taddress_id = [('Get Directions' in i) for i in whole_text].index(True)\n",
    "\t\taddress = whole_text[address_id].replace(\"Get Directions\",\"\")\n",
    "\texcept:\n",
    "\t\taddress =''\n",
    "\n",
    "    #Update address details in MongoDB collection\n",
    "\tcol.update_one({\"searchRank\":rank}, {\"$set\":{\"Shop Address\":address}})\n",
    "        \n",
    "\tapiURL = \"http://api.positionstack.com/v1/forward?access_key=c18a3f5e39fab26597545ffdcf4e8291&query=\" + address\n",
    "\tr= requests.get(apiURL)\n",
    "\tdata = r.json()\n",
    "\tlatitude = data[\"data\"][0]['latitude']\n",
    "\tlongitude = data[\"data\"][0]['longitude']\n",
    "\tgeoLocation = [latitude, longitude]\n",
    "\n",
    "    #Update search rank in MongoDB collection\n",
    "\tcol.update_one({\"searchRank\":rank}, {\"$set\":{\"Geo Location\":geoLocation}})\n",
    "\n",
    "    #Print shop details available on Donut shop page in console\n",
    "\tprint(\"Shop: \", rank)\n",
    "\tprint(\"Shop Website: \", website)\n",
    "\tprint(\"Shop Phone: \", phone_number)\n",
    "\tprint(\"Shop Address: \", address)\n",
    "\tprint(\"Geo Location: \", geoLocation)\n",
    "\tprint(\"\\n\")\n",
    "    \n",
    "##Adding index on search rank\n",
    "col.create_index('searchRank')\n",
    "\n",
    "#Close MongoDB client\n",
    "client.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display final collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to MongoDB client\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['sf_donut_shops']\n",
    "col = db['sf_donut_shops']\n",
    "\n",
    "col.create_index('searchRank')\n",
    "print(col.index_information())\n",
    "\n",
    "query = {}\n",
    "\n",
    "cursor = col.find(query)\n",
    "try:\n",
    "\tfor doc in cursor:\n",
    "\t\tprint(doc)\n",
    "finally:\n",
    "\tclient.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
